{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7af8c0ff",
   "metadata": {},
   "source": [
    "### Train Model with Sport Road Segments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "417a5980",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.03205798046867838\n",
      "R-squared Score: 0.010689356253126392\n",
      "\n",
      "Model Coefficients:\n",
      "Latitude: -0.3468965830431825\n",
      "Longitude: -0.06419372620333456\n",
      "Distance: -0.008540610405771127\n",
      "Sport_Basketball: 15255186128.26437\n",
      "Sport_Football: 15255186128.26265\n",
      "Sport_Hockey: 15255186128.260187\n",
      "Sport_Volleyball: 15255186128.263088\n",
      "Timestamp_010000PM: -5869447595.3067\n",
      "Timestamp_011500PM: -5869447595.3064995\n",
      "Timestamp_013000PM: -5869447595.309046\n",
      "Timestamp_020000PM: -5869447595.308961\n",
      "Timestamp_023000PM: -5869447595.30784\n",
      "Timestamp_030000PM: -5869447595.309719\n",
      "Timestamp_033000PM: -5869447595.312433\n",
      "Timestamp_040000PM: -5869447595.30879\n",
      "Timestamp_041500PM: -5869447595.312052\n",
      "Timestamp_043000PM: -5869447595.31079\n",
      "Timestamp_050000PM: -5869447595.308671\n",
      "Timestamp_053000PM: -5869447595.311526\n",
      "Timestamp_060000PM: -5869447595.309497\n",
      "Timestamp_063000PM: -5869447595.3110075\n",
      "Timestamp_070000PM: -5869447595.309279\n",
      "Timestamp_071500PM: -5869447595.309491\n",
      "Timestamp_073000PM: -5869447595.308567\n",
      "Timestamp_080000PM: -5869447595.307906\n",
      "Timestamp_081500PM: -5869447595.303741\n",
      "Timestamp_083000PM: -5869447595.304666\n",
      "Timestamp_090000PM: -5869447595.309775\n",
      "Timestamp_100000AM: -5869447595.359103\n",
      "Timestamp_110000AM: -5869447595.306499\n",
      "Timestamp_113000AM: -5869447595.30846\n",
      "Timestamp_120000PM: -5869447595.307458\n",
      "Timestamp_123000PM: -5869447595.306471\n",
      "Weekday?_False: -18628969351.93766\n",
      "Weekday?_True: -18628969351.93777\n",
      "Intercept: 9243230828.667166\n"
     ]
    }
   ],
   "source": [
    "#import statements\n",
    "import pandas as pd\n",
    "#train_test_split seperates the data into a training subset and a testing subset (two bins)\n",
    "##training is what creeates the machine learning model\n",
    "##testing is what we use for accuracy assessment\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Linear regression is our model type\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#mean_squared_error and r2_score are our two ways of evaluating accuracy\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "#OneHotEncoder --> This converts categorical data (i.e. sport type) into 'indicator' columns\n",
    "##indicator columns are yes/no for each category in the datatype\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#read in our data that is a 20% sample of the BIG dataset\n",
    "data = pd.read_csv('SampledData.csv')\n",
    "\n",
    "#we have a bad type of sport (line) --> remove it\n",
    "data_filtered = data[data['Sport'] != 'line']\n",
    "\n",
    "#categorical columns. We need to OneHotEncode these\n",
    "cat_columns = ['Sport', 'Timestamp', 'Weekday?']\n",
    "\n",
    "#we are transforming the data from having one column for each 'sport'/'timestamp'/'Weekday?' to multiple columns\n",
    "##Ex. There will be 4 columns for sport now. 'sport_Football', 'sport_hockey', 'sport_volleyball', 'sport_basketball'\n",
    "##These 4 columns will take 1/0 for yes/no if the column is that type. \n",
    "encoder = OneHotEncoder()\n",
    "encoded_columns = pd.DataFrame(encoder.fit_transform(data_filtered[cat_columns]).toarray(), index=data_filtered.index)\n",
    "encoded_columns.columns = encoder.get_feature_names_out(cat_columns)\n",
    "\n",
    "#numerical columns --> numbers that do not need any special processing\n",
    "num_columns = data_filtered[['Latitude', 'Longitude', 'Distance']]\n",
    "\n",
    "#put all our columns together\n",
    "X = pd.concat([num_columns, encoded_columns], axis=1)\n",
    "\n",
    "#y is our response column aka what we want our machine learning model to predict\n",
    "y = data_filtered['delta_cost']\n",
    "\n",
    "#split the dataset into training and testing datasets (70% training, 30% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "#create our model type\n",
    "model = LinearRegression()\n",
    "\n",
    "#train our model on the 'training' subset of our data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#make predictions on the 'testing' set of data so we can do accuracy assessment\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#accuracy assessment\n",
    "##get MSE using function comparing testing data to what the model predicts\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "##get R2 using function comparing testing data to what the model predicts\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "#print the evaluation metrics\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared Score:\", r2)\n",
    "\n",
    "#print the coefficients and intercept of the model\n",
    "##coeficients are slope for each variable\n",
    "print(\"\\nModel Coefficients:\")\n",
    "for feature, coef in zip(X.columns, model.coef_):\n",
    "    print(f\"{feature}: {coef}\")\n",
    "print(\"Intercept:\", model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90571f6f",
   "metadata": {},
   "source": [
    "### Applying the model in the real world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdeead81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we input what a user might want for 'Sport', 'Time', and 'Weekday'\n",
    "input_data = pd.DataFrame({\n",
    "    'Sport': ['Football'],\n",
    "    'Timestamp': ['070000PM'],\n",
    "    'Weekday?': [False]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2eb164",
   "metadata": {},
   "source": [
    "### Getting the base dataset ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25dfc8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function reads in our csv file and converts it to a dictionary\n",
    "def csv_to_dicts(csv_file):\n",
    "    df = pd.read_csv(csv_file, usecols=['Latitude', 'Longitude', 'geometry_wkt', 'Distance'])\n",
    "    data_dicts = df.to_dict(orient='records')\n",
    "    return data_dicts\n",
    "\n",
    "#convert our 'Base' data to a dictionary\n",
    "##Our base data is the baseline road dataset with 'SourceOID', 'Latitude', 'Longitude', 'geometry_wkt'\n",
    "data_dicts = csv_to_dicts('ML_Base_data.csv')\n",
    "\n",
    "#create an empty list to store our data\n",
    "data_frames = []\n",
    "\n",
    "#We are going to look at each row of data in the dictionary (each road segment)\n",
    "for data_dict in data_dicts:\n",
    "    #pull latitude, longitude, geometry\n",
    "    latitude = data_dict['Latitude']\n",
    "    longitude = data_dict['Longitude']\n",
    "    wkt = data_dict['geometry_wkt']\n",
    "    dist = data_dict['Distance']\n",
    "    #create a copy of our input_data and then add in new rows for latitude, longitude, and geometry\n",
    "    new_data = input_data.copy()\n",
    "    new_data['Latitude'] = latitude\n",
    "    new_data['Longitude'] = longitude\n",
    "    new_data['wkt'] = wkt\n",
    "    new_data['Distance'] = dist\n",
    "    #add the new row of data to our list\n",
    "    data_frames.append(new_data)\n",
    "\n",
    "#turn all the rows in one dataframe\n",
    "output_data = pd.concat(data_frames, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1012bb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that uses our machine learning model to predict a new 'delta_cost'\n",
    "def predict_delta_cost(input_data, encoder, model):\n",
    "    #use OneHotEncoder to turn our prediction data into multiple categorical columns\n",
    "    encoded_input = pd.DataFrame(encoder.transform(input_data[cat_columns]).toarray(), index=input_data.index)\n",
    "    #pulls the categorical columns from when we trained our machine learning model\n",
    "    encoded_input.columns = encoder.get_feature_names_out(cat_columns)\n",
    "    \n",
    "    #combine our OneHotEncoding with our latitude and longitude numerical columns\n",
    "    input_features = pd.concat([input_data[['Latitude', 'Longitude', 'Distance']], encoded_input], axis=1)\n",
    "    \n",
    "    #Make predictions of delta cost\n",
    "    predicted_delta_cost = model.predict(input_features)\n",
    "    \n",
    "    #add predicted delta cost to our original (not onehotencode) dataframe\n",
    "    input_data['delta_cost'] = predicted_delta_cost\n",
    "\n",
    "    return input_data\n",
    "\n",
    "#load dataframe from above\n",
    "input_data = output_data\n",
    "\n",
    "#call the prediction function\n",
    "output_data_delta = predict_delta_cost(input_data, encoder, model)\n",
    "#output our predicted delta costs to a csv file\n",
    "output_data_delta.to_csv('outputtestdelta3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9b9f7e-342a-4d7a-afcd-55088f2a5357",
   "metadata": {},
   "source": [
    "## Add our predictions to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c9087c-6b59-4b41-b326-c4803fb67a3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n"
     ]
    }
   ],
   "source": [
    "#import psycopg2 to work with database\n",
    "import psycopg2\n",
    "\n",
    "#define our connection stuff to database\n",
    "dbname = 'gis5572'\n",
    "user = 'postgres'\n",
    "host = '35.188.97.184'\n",
    "password = 'Passwordd'\n",
    "table_name = 'delta_cost_map3'\n",
    "\n",
    "#connect to database\n",
    "conn = psycopg2.connect(dbname=dbname, user=user, host=host, password=password)\n",
    "#create a cursor\n",
    "cur = conn.cursor()\n",
    "\n",
    "#drop the existing table if it exists\n",
    "cur.execute(f\"DROP TABLE IF EXISTS {table_name};\")\n",
    "\n",
    "#create a new table to store our data\n",
    "cur.execute(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    delta_cost NUMERIC,\n",
    "    geom GEOMETRY(LINESTRING, 4326)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "#commit changes to ensure the table is created\n",
    "conn.commit()\n",
    "\n",
    "#load our CSV file with data and delta values\n",
    "df = pd.read_csv('outputtestdelta3.csv')\n",
    "\n",
    "#count to know how many rows we are\n",
    "count = 1\n",
    "#loop and look at every row of data\n",
    "for index, row in df.iterrows():\n",
    "    print(count)\n",
    "    count += 1\n",
    "    #extract the wkt geometry and wkt values\n",
    "    geom = row['wkt']\n",
    "    delta_cost = row['delta_cost']\n",
    "    id = count\n",
    "    \n",
    "    #insert data into the database, converting WKT to geometry\n",
    "    cur.execute(f\"INSERT INTO {table_name} (delta_cost, geom) VALUES (%s, ST_GeomFromText(%s, 4326))\", (delta_cost, geom))\n",
    "\n",
    "    #commit (tell database data is ready) every 5000 iterations \n",
    "    ##just so there is data being added to database continuously instead of all 52000 rows at once\n",
    "    if count%5000 == 0:\n",
    "        conn.commit()\n",
    "\n",
    "#commit the last amount of data\n",
    "conn.commit()\n",
    "\n",
    "#close the cursor and connection to clean up\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86be1afb-85f1-403f-adb2-d87737fcde8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d64a52d4-08ff-45cb-9aa5-66278972726f",
   "metadata": {},
   "source": [
    "# Add to app.py for displaying on arcgis online MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e0f95dd-d4eb-404e-be89-257a3af1f62c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'app' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Route to retrieve polygon as GeoJSON\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;129m@app\u001b[39m\u001b[38;5;241m.\u001b[39mroute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/delta_cost_map\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdelta_cost_map\u001b[39m():\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Connect to the database\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     conn \u001b[38;5;241m=\u001b[39m psycopg2\u001b[38;5;241m.\u001b[39mconnect(\n\u001b[0;32m      6\u001b[0m         dbname\u001b[38;5;241m=\u001b[39mDB_NAME,\n\u001b[0;32m      7\u001b[0m         user\u001b[38;5;241m=\u001b[39mDB_USER,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m         port\u001b[38;5;241m=\u001b[39mDB_PORT\n\u001b[0;32m     11\u001b[0m     )\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# Create a cursor\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'app' is not defined"
     ]
    }
   ],
   "source": [
    "# Route to retrieve polygon as GeoJSON\n",
    "@app.route('/delta_cost_map3')\n",
    "def delta_cost_map():\n",
    "    # Connect to the database\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=DB_NAME,\n",
    "        user=DB_USER,\n",
    "        password=DB_PASSWORD,\n",
    "        host=DB_HOST,\n",
    "        port=DB_PORT\n",
    "    )\n",
    "\n",
    "    # Create a cursor\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Execute SQL query to retrieve the polygon\n",
    "    cur.execute(\"\"\"SELECT \n",
    "                    json_build_object(\n",
    "                        'type', 'FeatureCollection',\n",
    "                        'features', json_agg(\n",
    "                            json_build_object(\n",
    "                                'type', 'Feature',\n",
    "                                'geometry', ST_AsGeoJSON(ST_SetSRID(geom, 4326))::json,\n",
    "                                'properties', json_build_object(\n",
    "                                    'delta_cost', delta_cost\n",
    "                                )\n",
    "                            )\n",
    "                        ),\n",
    "                        'crs', \n",
    "                        json_build_object(\n",
    "                            'type', 'name',\n",
    "                            'properties', \n",
    "                            json_build_object(\n",
    "                                'name', 'EPSG:4326'\n",
    "                            )\n",
    "                        )\n",
    "                    ) AS geojson\n",
    "                FROM delta_cost_map3;\n",
    "                \"\"\")\n",
    "    rows = cur.fetchone()[0]\n",
    "\n",
    "    # Close cursor and connection\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "    # Return the GeoJSON\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93369627",
   "metadata": {},
   "source": [
    "### User Selection (DOES NOT WORK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb428e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
